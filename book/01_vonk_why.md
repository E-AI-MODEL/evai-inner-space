# Hoofdstuk 1: De Vonk

> *"Waarom zou je een brein willen bouwen dat je kunt begrijpen?"*

---

## 1. Het Moment van Frustratie

Het was 2023. Ik zat tegenover mijn scherm en typte een vraag aan een AI-chatbot: *"Ik voel me angstig."* Het antwoord kwam binnen seconden: een empathisch klinkende response, vol begrip en suggesties. Maar één vraag bleef me dwarszitten:

**Waarom gaf de AI dit specifieke antwoord?**

Ik kon het systeem niet vragen. Er was geen knop om te klikken, geen dashboard om te bekijken, geen trail van redenering om te volgen. Het was een **black box**: input ging erin, output kwam eruit, en wat ertussenin gebeurde was verborgen achter miljarden parameters.

Dit was niet acceptabel. Niet voor een systeem dat mensen zou moeten helpen met hun emoties.

---

## 2. De Droom van Uitlegbaarheid

Ik begon met een simpele vraag: **Wat als we een AI zouden kunnen bouwen die zichzelf kan uitleggen?**

Niet in algemene termen zoals "Ik gebruikte neurale netwerken", maar concreet:
- Welke kennis werd gebruikt om deze response te genereren?
- Hoe zeker is het systeem van dit antwoord?
- Welke therapeutische principes werden toegepast?
- Wat zou er gebeuren als de context anders was?

Deze transparantie was niet alleen een **technische uitdaging**, maar ook een **morele imperatief**. Als AI mensen zou helpen met hun mentale gezondheid, moesten we kunnen vertrouwen op de redenering erachter.

---

## 3. Waarom Emotionele Zorg?

Van alle domeinen die ik kon kiezen, koos ik het moeilijkste: **emotionele ondersteuning**.

### Waarom dit domein zo complex is:
1. **Subjectiviteit**: Emoties zijn persoonlijk en contextueel
2. **Risico**: Fouten kunnen schade aanrichten (crisis situaties)
3. **Vertrouwen**: Gebruikers moeten zich veilig voelen
4. **Nuance**: Een "simpel" antwoord kan contraproductief zijn
5. **Ethiek**: De grens tussen ondersteuning en behandeling

Als ik transparantie kon bereiken in dit domein, zou het principe overal toepasbaar zijn.

---

## 4. Het Filosofische Probleem

Er is een paradox in AI-transparantie:

```
Hoe meer parameters een model heeft,
hoe beter het presteert,
maar hoe minder we begrijpen waarom.
```

**GPT-4** heeft naar schatting 1.7 biljoen parameters. Zelfs de beste explainable AI (XAI) methoden kunnen slechts oppervlakkig inzicht geven in zo'n systeem. Attention maps, gradient visualisaties, SHAP values - ze vertellen je *wat* het model doet, niet *waarom* het zinvol is.

Dit leidde tot mijn centrale hypothese:

> **"Wat als we een systeem bouwen dat van nature uitlegbaar is,  
> in plaats van achteraf uitleg te proberen te extraheren?"**

---

## 5. De Inspiratie: Het Menselijk Brein

Ons brein werkt niet als één groot neuraal netwerk. Het is een **gelaagd systeem**:

- **Thalamus**: Sensory input filtering
- **Amygdala**: Emotie-detectie (snel, onbewust)
- **Hippocampus**: Geheugen en context
- **Prefrontale cortex**: Redenering en besluitvorming
- **Broca/Wernicke**: Taal productie en begrip

Elk niveau heeft een **specifieke functie** en samen vormen ze een coherent geheel. Belangrijker nog: we kunnen *redeneren* over elk niveau. We weten waarom we een snelle emotionele reactie hebben (amygdala) en hoe we die moduleren met rationeel denken (prefrontale cortex).

Dit werd mijn model: **een AI die denkt in lagen**, waarbij elke laag een cognitieve functie vertegenwoordigt en uitlegbaar is.

---

## 6. Van Idee naar Systeem

Het duurde maanden van prototypes, mislukte pogingen en doorbraken voordat het concept vorm kreeg. Maar het kernprincipe bleef overeind:

### De 7 Lagen van EvAI Inner Space
1. **Safety Check**: Voorkomt harmful content
2. **Rubrics Assessment**: Therapeutische risicobeoordeling
3. **Strategic Briefing**: Conversational strategie
4. **Browser ML Emotion**: Lokale emotie-detectie
5. **Unified Knowledge Search**: Symbolisch + Semantisch + Neuraal
6. **Hybrid Ranking**: Combineer alle bronnen
7. **Self-Learning**: Automatische kennisgeneratie

Elke laag is **aantoonbaar**, **traceerbaar**, en **uitlegbaar**.

---

## 7. Het Persoonlijke Verhaal

Waarom was dit zo belangrijk voor mij?

Ik heb zelf ervaring met therapy. Ik weet hoe kwetsbaar je je voelt als je je emoties deelt met iemand (of iets). Het idee dat een black box AI zou beslissen hoe te reageren op mijn diepste angsten was onacceptabel.

**Vertrouwen** komt niet van perfectie, maar van **transparantie**. Ik wil weten:
- Wat weet het systeem over mijn emotie?
- Waar komt die kennis vandaan?
- Hoe zeker is het van zijn antwoord?
- Wat zou er gebeuren als ik iets anders had gezegd?

Dit boek is mijn antwoord op die vragen. Het is een blauwdruk voor **AI die we kunnen begrijpen**.

---

## 8. De Reis Begint

In de volgende hoofdstukken neem ik je mee door:
- Het probleem van black box AI (H2)
- De neurosymbolische hypothese (H3)
- De technische implementatie (H4-8)
- De evaluatie en ethiek (H9-12)

Dit is niet alleen een verhaal over code. Het is een verhaal over **vertrouwen**, **verantwoordelijkheid**, en de vraag: **Kunnen we AI bouwen die niet alleen slim is, maar ook begrijpelijk?**

---

## Reflectie: Wat ik Geleerd Heb

Na 2 jaar ontwikkeling kan ik zeggen: **Ja, het is mogelijk.**

Maar het vereist:
- **Bescheidenheid**: Erkennen wat we niet weten
- **Structuur**: Lagen en modules in plaats van één groot model
- **Transparantie**: Elk besluit moet aantoonbaar zijn
- **Menselijkheid**: Technologie dient mensen, niet andersom

De vonk die dit project startte was frustratie. Wat het in leven houdt is **hoop**: hoop dat we AI kunnen bouwen die niet alleen helpt, maar ook uitlegt *waarom*.

---

**Volgende**: [H2 - De Onmogelijke Opgave](02_onmogelijke_opgave.md)
